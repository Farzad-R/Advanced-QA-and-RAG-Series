{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Tools and Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_notebook_config import LoadConfig\n",
    "from tools.tools_lookup_policy import lookup_policy\n",
    "from tools.tools_flights import fetch_user_flight_information, search_flights, update_ticket_to_new_flight, cancel_ticket\n",
    "from tools.tools_hotels import search_hotels, book_hotel, update_hotel, cancel_hotel\n",
    "from tools.tools_excursions import search_trip_recommendations, book_excursion, update_excursion, cancel_excursion\n",
    "from tools.tools_car_rental import search_car_rentals, book_car_rental, update_car_rental, cancel_car_rental\n",
    "from utils.utilities import create_tool_node_with_fallback, _print_event\n",
    "\n",
    "CFG = LoadConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Assistant:** This class takes the graph state, formats it into a prompt, and then calls an LLM for it to predict the best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Primary Agent's System Role**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrap Up the Tools and Bind Them to the Primary Agent (LLM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    fetch_user_flight_information,\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "    update_ticket_to_new_flight,\n",
    "    cancel_ticket,\n",
    "    search_car_rentals,\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "    search_hotels,\n",
    "    book_hotel,\n",
    "    update_hotel,\n",
    "    cancel_hotel,\n",
    "    search_trip_recommendations,\n",
    "    book_excursion,\n",
    "    update_excursion,\n",
    "    cancel_excursion,\n",
    "]\n",
    "assistant_runnable = primary_assistant_prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Graph**\n",
    "\n",
    "Now, create the graph, incorporating two adjustments from Part 1 to address our earlier concerns:\n",
    "\n",
    "1. Insert an interrupt before utilizing a tool.\n",
    "2. Populate the user state explicitly within the first node, ensuring the assistant doesn't need to use a tool solely to gather information about the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": fetch_user_flight_information.invoke({})}\n",
    "\n",
    "\n",
    "# NEW: The fetch_user_info node runs first, meaning our assistant can see the user's flight information without\n",
    "# having to take an action\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.add_edge(START, \"fetch_user_info\")\n",
    "builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
    "builder.add_edge(\"fetch_user_info\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # NEW: The graph will always halt before executing the \"tools\" node.\n",
    "    # The user can approve or reject (or even alter the request) before\n",
    "    # the assistant continues\n",
    "    interrupt_before=[\"tools\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an example conversation a user might have with the assistant\n",
    "sample_questions = [\n",
    "    # \"Hi there, what time is my flight?\",\n",
    "    # \"Am i allowed to update my flight to something sooner? I want to leave later today.\",\n",
    "    \"Update my flight to sometime next week then\",\n",
    "    # \"The next available option is great\",\n",
    "    # \"what about lodging and transportation?\",\n",
    "    # \"Yeah i think i'd like an affordable hotel for my week-long stay (7 days). And I'll want to rent a car.\",\n",
    "    # \"OK could you place a reservation for your recommended hotel? It sounds nice.\",\n",
    "    # \"yes go ahead and book anything that's moderate expense and has availability.\",\n",
    "    # \"Now for a car, what are my options?\",\n",
    "    # \"Awesome let's just get the cheapest option. Go ahead and book for 7 days\",\n",
    "    # \"Cool so now what recommendations do you have on excursions?\",\n",
    "    # \"Are they available while I'm there?\",\n",
    "    # \"interesting - i like the museums, what options are there? \",\n",
    "    # \"OK great pick one and book it for my second day there.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the Databases and Configure Settings for a Sample User**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utilities import update_dates\n",
    "import uuid\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "db = update_dates(CFG.local_file, CFG.backup_file)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the Bot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "_printed = set()\n",
    "for question in sample_questions:\n",
    "    events = graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt! The agent is trying to use a tool, and the user can approve or deny it\n",
    "        # Note: This code is all outside of your graph. Typically, you would stream the output to a UI.\n",
    "        # Then, you would have the frontend trigger a new run via an API call when the user has provided input.\n",
    "        try:\n",
    "            user_input = input(\n",
    "                \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "                \" otherwise, explain your requested changed.\\n\\n\"\n",
    "            )\n",
    "        except:\n",
    "            user_input = \"y\"\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "            result = graph.invoke(\n",
    "                None,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by\n",
    "            # providing instructions on the requested changes / change of mind\n",
    "            result = graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "        snapshot = graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot[0][\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "## **Performance**\n",
    "\n",
    "Now our assistant was able to save a step to respond with our flight details. We also completely controlled which actions were performed. This all worked using LangGraph's interrupts and checkpointers.\n",
    "\n",
    "## **Problems**\n",
    "We are involving the user in EVERY assistant action which is not necessary.\n",
    "\n",
    "------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airline-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
