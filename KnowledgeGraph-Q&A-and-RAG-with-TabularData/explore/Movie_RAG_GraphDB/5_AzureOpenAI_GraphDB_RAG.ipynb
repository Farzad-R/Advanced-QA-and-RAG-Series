{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "load_dotenv()\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the environment variables from your `.env` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "azure_openai_endpoint = os.environ[\"OPENAI_API_BASE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Azure openAI instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = azure_openai_api_key,  \n",
    "  api_version = \"2023-07-01-preview\",\n",
    "  azure_endpoint = azure_openai_endpoint\n",
    ")\n",
    "\n",
    "def embed_text(text:str)->List:\n",
    "    \"\"\"\n",
    "    Embeds the given text using the specified model.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be embedded.\n",
    "\n",
    "    Returns:\n",
    "        List: A list containing the embedding of the text.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "    input = text,\n",
    "    model= \"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Neo4j credentials (These information need to be kept secret)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"12345678\"\n",
    "NEO4J_DATABASE = 'neo4j'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample question for RAG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What movies are about crime?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the questions embedding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.023391522467136383,\n",
       " -0.032135896384716034,\n",
       " 0.001850268105044961,\n",
       " -0.03603663668036461,\n",
       " -0.030766762793064117,\n",
       " 0.019232455641031265,\n",
       " 0.007162119727581739,\n",
       " -0.01681709662079811,\n",
       " -0.006209538783878088,\n",
       " -0.017695408314466476]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding = embed_text(question)\n",
    "question_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Similarity Search using the question's embedding on the vector index of the graph database and get the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'movie.title': 'Heat',\n",
       "  'movie.tagline': 'A Los Angeles crime saga',\n",
       "  'score': 0.9320709109306335},\n",
       " {'movie.title': 'Tom and Huck',\n",
       "  'movie.tagline': 'The Original Bad Boys.',\n",
       "  'score': 0.9016362428665161},\n",
       " {'movie.title': 'Balto',\n",
       "  'movie.tagline': 'Part Dog. Part Wolf. All Hero.',\n",
       "  'score': 0.8778379559516907}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding      // Use the provided question embedding as 'question_embedding'\n",
    "    CALL db.index.vector.queryNodes(                    // Call the vector index query function\n",
    "        'movie_tagline_embeddings',                     // Name of the vector index to query against\n",
    "        $top_k,                                         // Number of top results to retrieve\n",
    "        question_embedding                              // The question embedding to compare against\n",
    "        ) YIELD node AS movie, score                    // Yield each matched node and its similarity score\n",
    "    RETURN movie.title, movie.tagline, score            // Return the title, tagline, and similarity score of each movie\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,       # Pass the question embedding as a parameter\n",
    "        \"top_k\": 3                                      # Specify the number of top results to retrieve\n",
    "    })\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pass the results to an LLM for the final answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movies about crime are \"Heat\" and \"Tom and Huck\".\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": str(\n",
    "        \"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"gpt_deployment_name\"),\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "**Note: In this usecase, there is a higher chance of hallucination due to lack of enough evidence for the LLM to use its own judgment. The contents of the vector DB and the system role can address this issue to some extent.**\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second example (in one go):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movies about adventure are:\n",
      "1. Toy Story - \"The adventure takes off!\"\n",
      "2. Cutthroat Island - \"The Course Has Been Set. There Is No Turning Back. Prepare Your Weapons. Summon Your Courage. Discover the Adventure of a Lifetime!\"\n",
      "3. Tom and Huck - \"The Original Bad Boys.\"\n",
      "4. Jumanji - \"Roll the dice and unleash the excitement!\"\n"
     ]
    }
   ],
   "source": [
    "question = \"What movies are about adventure?\"\n",
    "question_embedding = embed_text(question)\n",
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding\n",
    "    CALL db.index.vector.queryNodes(\n",
    "        'movie_tagline_embeddings', \n",
    "        $top_k, \n",
    "        question_embedding\n",
    "        ) YIELD node AS movie, score\n",
    "    RETURN movie.title, movie.tagline, score\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,\n",
    "        \"top_k\": 5\n",
    "    })\n",
    "\n",
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": str(\n",
    "        \"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"gpt_deployment_name\"),\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playaround",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
