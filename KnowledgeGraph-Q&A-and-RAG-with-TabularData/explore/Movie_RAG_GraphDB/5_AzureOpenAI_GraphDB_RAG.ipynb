{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "load_dotenv()\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the environment variables from your `.env` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "azure_openai_endpoint = os.environ[\"OPENAI_API_BASE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Azure openAI instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  api_key = azure_openai_api_key,  \n",
    "  api_version = \"2023-07-01-preview\",\n",
    "  azure_endpoint = azure_openai_endpoint\n",
    ")\n",
    "\n",
    "def embed_text(text:str)->List:\n",
    "    \"\"\"\n",
    "    Embeds the given text using the specified model.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be embedded.\n",
    "\n",
    "    Returns:\n",
    "        List: A list containing the embedding of the text.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "    input = text,\n",
    "    model= \"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Neo4j credentials (These information need to be kept secret)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"12345678\"\n",
    "NEO4J_DATABASE = 'neo4j'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample question for RAG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What movies are about love?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the questions embedding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004219826776534319,\n",
       " -0.03029683791100979,\n",
       " 0.0007639749674126506,\n",
       " -0.02761838585138321,\n",
       " -0.007965870201587677,\n",
       " 0.0019409307278692722,\n",
       " -0.006576106883585453,\n",
       " -0.021617135033011436,\n",
       " -0.002027790993452072,\n",
       " -0.008066943846642971]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding = embed_text(question)\n",
    "question_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Similarity Search using the question's embedding on the vector index of the graph database and get the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'movie.title': 'Heat',\n",
       "  'movie.tagline': 'A Los Angeles crime saga',\n",
       "  'score': 0.8910084962844849},\n",
       " {'movie.title': 'Grumpier Old Men',\n",
       "  'movie.tagline': 'Still Yelling. Still Fighting. Still Ready for Love.',\n",
       "  'score': 0.8878611922264099},\n",
       " {'movie.title': 'Tom and Huck',\n",
       "  'movie.tagline': 'The Original Bad Boys.',\n",
       "  'score': 0.8836773633956909}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding      // Use the provided question embedding as 'question_embedding'\n",
    "    CALL db.index.vector.queryNodes(                    // Call the vector index query function\n",
    "        'movie_tagline_embeddings',                     // Name of the vector index to query against\n",
    "        $top_k,                                         // Number of top results to retrieve\n",
    "        question_embedding                              // The question embedding to compare against\n",
    "        ) YIELD node AS movie, score                    // Yield each matched node and its similarity score\n",
    "    RETURN movie.title, movie.tagline, score            // Return the title, tagline, and similarity score of each movie\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,       # Pass the question embedding as a parameter\n",
    "        \"top_k\": 3                                      # Specify the number of top results to retrieve\n",
    "    })\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pass the results to an LLM for the final answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movies that are about love are \"Heat\", \"Grumpier Old Men\" and \"Tom and Huck\".\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": str(\n",
    "        \"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"gpt_deployment_name\"),\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "\n",
    "**Note: In this usecase, there is a higher chance of hallucination due to lack of enough evidence for the LLM to use its own judgment. The contents of the vector DB and the system role can address this issue to some extent.**\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second example (in one go):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movies about adventure are:\n",
      "- Toy Story \n",
      "- Cutthroat Island \n",
      "- Tom and Huck \n",
      "- Jumanji\n"
     ]
    }
   ],
   "source": [
    "question = \"What movies are about adventure?\"\n",
    "question_embedding = embed_text(question)\n",
    "result = graph.query(\"\"\"\n",
    "    with $question_embedding as question_embedding\n",
    "    CALL db.index.vector.queryNodes(\n",
    "        'movie_tagline_embeddings', \n",
    "        $top_k, \n",
    "        question_embedding\n",
    "        ) YIELD node AS movie, score\n",
    "    RETURN movie.title, movie.tagline, score\n",
    "    \"\"\",\n",
    "    params={\n",
    "        \"question_embedding\": question_embedding,\n",
    "        \"top_k\": 5\n",
    "    })\n",
    "\n",
    "prompt = f\"# Question:\\n{question}\\n\\n# Graph DB search results:\\n{result}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": str(\n",
    "        \"You will be given the user question along with the search result of that question over a Neo4j graph database. Give the user the proper answer.\"\n",
    "    )},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"gpt_deployment_name\"),\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playaround",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
